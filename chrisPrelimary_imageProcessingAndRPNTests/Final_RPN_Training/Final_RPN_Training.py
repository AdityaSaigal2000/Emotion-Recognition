# -*- coding: utf-8 -*-
"""Train Function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17q37YBQvo5Q0N5jClqXwjUS9kyQ7Yrla
"""

from google.colab import drive
drive.mount('/content/gdrive')

import sys
sys.path.append('/content/gdrive/My Drive/APS360ProjectSharedData/Emotion-Recognition-master')

cd /content/gdrive/My Drive

import os
import scipy.io as sio
import pickle
import uuid
import numpy as np
import PIL
import scipy.sparse
import torch
from torch.utils.data.sampler import Sampler
import cv2

class imdb(object):
  """Image database."""

  def __init__(self, name, classes=None):
    self._name = name
    self._num_classes = 0
    if not classes:
      self._classes = []
    else:
      self._classes = classes
    self._image_index = []
    self._obj_proposer = 'gt'
    self._roidb = None
    self._roidb_handler = self.default_roidb
    # Use this dict for storing dataset specific config options
    self.config = {}

  @property
  def name(self):
    return self._name

  @property
  def num_classes(self):
    return len(self._classes)

  @property
  def classes(self):
    return self._classes

  @property
  def image_index(self):
    return self._image_index

  @property
  def roidb_handler(self):
    return self._roidb_handler

  @roidb_handler.setter
  def roidb_handler(self, val):
    self._roidb_handler = val

  def set_proposal_method(self, method):
    method = eval('self.' + method + '_roidb')
    self.roidb_handler = method

  @property
  def roidb(self):
    # A roidb is a list of dictionaries, each with the following keys:
    #   boxes
    #   gt_overlaps
    #   gt_classes
    #   flipped
    if self._roidb is not None:
      return self._roidb
    self._roidb = self.roidb_handler()
    return self._roidb

  @property
  def cache_path(self):
    cache_path = os.path.abspath(os.path.join('data', 'cache'))
    if not os.path.exists(cache_path):
      os.makedirs(cache_path)
    return cache_path

  @property
  def num_images(self):
    return len(self.image_index)

  def image_path_at(self, i):
    raise NotImplementedError

  def image_id_at(self, i):
    raise NotImplementedError

  def default_roidb(self):
    raise NotImplementedError

class wider_face(imdb):
    def __init__(self, image_set):
        """
        WIDER Face data loader
        """
        name = 'wider_face_' + image_set
        imdb.__init__(self, name)
        self._devkit_path = self._get_default_path()  # ./data/WIDER2015
        # ./data/WIDER2015/WIDER_train/images
        self._data_path = os.path.join(self._devkit_path, 'WIDER_' + image_set, 'images')
        # Example path to image set file:
        image_set_file = os.path.join(self._devkit_path, 'wider_face_split', 'wider_face_' + image_set + '.mat')
        assert os.path.exists(image_set_file), \
            'Path does not exist: {}'.format(image_set_file)
        self._wider_image_set = sio.loadmat(image_set_file, squeeze_me=True)
        self._classes = ('__background__',  # always index 0
                         'face')
        self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))
        self._image_ext = '.jpg'
        self._image_index, self._face_bbx = self._load_image_set_index()
        # Default to roidb handler
        self._roidb_handler = self.gt_roidb
        self._salt = str(uuid.uuid4())
        self._comp_id = 'comp4'

        # PASCAL specific config options
        self.config = {'cleanup': True,
                       'use_salt': True,
                       'matlab_eval': False,
                       'rpn_file': None}

        assert os.path.exists(self._devkit_path), \
            'VOCdevkit path does not exist: {}'.format(self._devkit_path)
        assert os.path.exists(self._data_path), \
            'Path does not exist: {}'.format(self._data_path)

    def image_path_at(self, i):
        """
        Return the absolute path to image i in the image sequence.
        """
        return self.image_path_from_index(self._image_index[i])

    def image_id_at(self, i):
        """
        Return the absolute path to image i in the image sequence.
        """
        return i

    def image_path_from_index(self, index):
        """
        Construct an image path from the image's "index" identifier.
        """
        image_path = os.path.join(self._data_path,
                                  index + self._image_ext)
        assert os.path.exists(image_path), \
            'Path does not exist: {}'.format(image_path)
        return image_path

    def _load_image_set_index(self):
        """
        Load the indexes listed in this dataset's image set file.
        """
        event_list = self._wider_image_set['event_list']
        file_list = self._wider_image_set['file_list']
        face_bbx_list = self._wider_image_set['face_bbx_list']
        image_index = []
        face_bbx = []
        for i in range(len(event_list)):
            for j in range(len(file_list[i])):
                image_index.append(str(event_list[i]) + '/' + str(file_list[i][j]))
                face_bbx.append(face_bbx_list[i][j].reshape(-1, 4))
        # _wider_image_set = np.concatenate(_wider_image_set['file_list']).ravel().tolist()
        # image_index = map(str, _wider_image_set)
        return image_index, face_bbx

    def _get_default_path(self):
        """
        Return the default path where PASCAL VOC is expected to be installed.
        """
        return os.path.join('data', 'WIDER2015')

    def gt_roidb(self):
        """
        Return the database of ground-truth regions of interest.
        This function loads/saves from/to a cache file to speed up future calls.
        """
        cache_file = os.path.join(self.cache_path, self.name + '_gt_roidb.pkl')
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as fid:
                try:
                    roidb = pickle.load(fid)
                except:
                    roidb = pickle.load(fid, encoding='bytes')
            print('{} gt roidb loaded from {}'.format(self.name, cache_file))
            return roidb

        gt_roidb = [self._load_wider_annotation(index)
                    for index in range(len(self.image_index))]
        with open(cache_file, 'wb') as fid:
            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)
        print('wrote gt roidb to {}'.format(cache_file))

        return gt_roidb

    def _load_wider_annotation(self, index):
        """
        Load image and bounding boxes info from XML file in the PASCAL VOC
        format.
        """
        imw, imh = PIL.Image.open(self.image_path_at(index)).size
        num_objs = self._face_bbx[index].shape[0]

        boxes = np.zeros((num_objs, 4), dtype=np.uint16)
        gt_classes = np.zeros((num_objs), dtype=np.int32)
        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)
        # "Seg" area for pascal is just the box area
        seg_areas = np.zeros((num_objs), dtype=np.float32)

        # Load object bounding boxes into a data frame.
        for ix in range(num_objs):
            assert not np.any(np.isnan(self._face_bbx[index][ix]))
            x1 = min(max(0, self._face_bbx[index][ix][0]), imw - 1)
            y1 = min(max(0, self._face_bbx[index][ix][1]), imh - 1)
            w = abs(self._face_bbx[index][ix][2])
            h = abs(self._face_bbx[index][ix][3])
            x2 = min(max(x1 + w, 0), imw - 1)
            y2 = min(max(y1 + h, 0), imh - 1)
            cls = 1
            boxes[ix, :] = [x1, y1, x2, y2]
            gt_classes[ix] = cls
            overlaps[ix, cls] = 1.0
            seg_areas[ix] = (w + 1) * (h + 1)

        overlaps = scipy.sparse.csr_matrix(overlaps)

        return {'boxes': boxes,
                'gt_classes': gt_classes,
                'gt_overlaps': overlaps,
                'flipped': False,
                'seg_areas': seg_areas}

    def _get_comp_id(self):
        comp_id = (self._comp_id + '_' + self._salt if self.config['use_salt']
                   else self._comp_id)
        return comp_id
    
class sampler(Sampler):
  def __init__(self, train_size, batch_size):
    self.num_data = train_size
    self.num_per_batch = int(train_size / batch_size)
    self.batch_size = batch_size
    self.range = torch.arange(0,batch_size).view(1, batch_size).long()
    self.leftover_flag = False
    if train_size % batch_size:
      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()
      self.leftover_flag = True

  def __iter__(self):
    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size
    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range

    self.rand_num_view = self.rand_num.view(-1)

    if self.leftover_flag:
      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)

    return iter(self.rand_num_view)

  def __len__(self):
    return self.num_data

class roibatchLoader(torch.utils.data.Dataset):
  def __init__(self, roidb, ratio_list, ratio_index, batch_size, num_classes, training=True, normalize=None):
    self._roidb = roidb
    self._num_classes = num_classes
    # we make the height of image consistent to trim_height, trim_width
    self.trim_height = 512
    self.trim_width = 512
    self.max_num_box = 20
    self.training = training
    self.normalize = normalize
    self.ratio_list = ratio_list
    self.ratio_index = ratio_index
    self.batch_size = batch_size
    self.data_size = len(self.ratio_list)

    # given the ratio_list, we want to make the ratio same for each batch.
    self.ratio_list_batch = torch.Tensor(self.data_size).zero_()
    num_batch = int(np.ceil(len(ratio_index) / batch_size))
    for i in range(num_batch):
        left_idx = i*batch_size
        right_idx = min((i+1)*batch_size-1, self.data_size-1)

        if ratio_list[right_idx] < 1:
            # for ratio < 1, we preserve the leftmost in each batch.
            target_ratio = ratio_list[left_idx]
        elif ratio_list[left_idx] > 1:
            # for ratio > 1, we preserve the rightmost in each batch.
            target_ratio = ratio_list[right_idx]
        else:
            # for ratio cross 1, we make it to be 1.
            target_ratio = 1

        self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio


  def __getitem__(self, index):
    if self.training:
        index_ratio = int(self.ratio_index[index])
    else:
        index_ratio = index

    # get the anchor index for current sample index
    # here we set the anchor index to the last one
    # sample in this group
    minibatch_db = [self._roidb[index_ratio]]
    blobs = get_minibatch(minibatch_db, self._num_classes)
    data = torch.from_numpy(blobs['data'])
    im_info = torch.from_numpy(blobs['im_info'])
    # we need to random shuffle the bounding box.
    data_height, data_width = data.size(1), data.size(2)
    if self.training:
        np.random.shuffle(blobs['gt_boxes'])
        gt_boxes = torch.from_numpy(blobs['gt_boxes'])

        ########################################################
        # padding the input image to fixed size for each group #
        ########################################################

        # NOTE1: need to cope with the case where a group cover both conditions. (done)
        # NOTE2: need to consider the situation for the tail samples. (no worry)
        # NOTE3: need to implement a parallel data loader. (no worry)
        # get the index range

        # if the image need to crop, crop to the target size.
        ratio = 1.0

        if data_width < data_height:
            min_y = int(torch.min(gt_boxes[:,1]))
            max_y = int(torch.max(gt_boxes[:,3]))
            trim_size = int(np.floor(data_width / ratio))
            if trim_size > data_height:
                trim_size = data_height                
            box_region = max_y - min_y + 1
            if min_y == 0:
                y_s = 0
            else:
                if (box_region-trim_size) < 0:
                    y_s_min = max(max_y-trim_size, 0)
                    y_s_max = min(min_y, data_height-trim_size)
                    if y_s_min == y_s_max:
                        y_s = y_s_min
                    else:
                        y_s = np.random.choice(range(y_s_min, y_s_max))
                else:
                    y_s_add = int((box_region-trim_size)/2)
                    if y_s_add == 0:
                        y_s = min_y
                    else:
                        y_s = np.random.choice(range(min_y, min_y+y_s_add))
            # crop the image
            data = data[:, y_s:(y_s + trim_size), :, :]

            # shift y coordiante of gt_boxes
            gt_boxes[:, 1] = gt_boxes[:, 1] - float(y_s)
            gt_boxes[:, 3] = gt_boxes[:, 3] - float(y_s)

            # update gt bounding box according the trip
            gt_boxes[:, 1].clamp_(0, trim_size - 1)
            gt_boxes[:, 3].clamp_(0, trim_size - 1)
            
            padding_data = data[0]

        elif data_width > data_height:
            trim_size = int(np.floor(data_width / ratio))

            padding_data = torch.FloatTensor(int(np.ceil(data_width / ratio)), \
                                             data_width, 3).zero_()

            padding_data[:data_height, :, :] = data[0]
            # update im_info
            im_info[0, 0] = padding_data.size(0)
        
        else:
            padding_data = data[0]


        # check the bounding box:
        not_keep = (gt_boxes[:,0] == gt_boxes[:,2]) | (gt_boxes[:,1] == gt_boxes[:,3])
        keep = torch.nonzero(not_keep == 0).view(-1)

        gt_boxes_padding = torch.FloatTensor(self.max_num_box, gt_boxes.size(1)).zero_()
        if keep.numel() != 0:
            gt_boxes = gt_boxes[keep]
            num_boxes = min(gt_boxes.size(0), self.max_num_box)
            gt_boxes_padding[:num_boxes,:] = gt_boxes[:num_boxes]
        else:
            num_boxes = 0

            # permute trim_data to adapt to downstream processing
        padding_data = padding_data.permute(2, 0, 1).contiguous()
        im_info = im_info.view(3)

        return padding_data, im_info, gt_boxes_padding, num_boxes
    else:
        data = data.permute(0, 3, 1, 2).contiguous().view(1, data_height, data_width)
        im_info = im_info.view(3)

        gt_boxes = torch.FloatTensor([1,1,1,1,1])
        num_boxes = 0

        return data, im_info, gt_boxes, num_boxes

  def __len__(self):
    return len(self._roidb)

def get_minibatch(roidb, num_classes):
  """Given a roidb, construct a minibatch sampled from it."""
  num_images = len(roidb)
  # Sample random scales to use for each image in this batch
  random_scale_inds = np.random.randint(0, high=1, size=num_images)

  # Get the input image blob, formatted for caffe
  im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)

  blobs = {'data': im_blob}

  assert len(im_scales) == 1, "Single batch only"
  assert len(roidb) == 1, "Single batch only"
  
  # gt boxes: (x1, y1, x2, y2, cls)
  gt_inds = np.where(roidb[0]['gt_classes'] != 0)[0]
  gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)
  gt_boxes[:, 0:4] = roidb[0]['boxes'][gt_inds, :] * im_scales[0]
  gt_boxes[:, 4] = roidb[0]['gt_classes'][gt_inds]
  blobs['gt_boxes'] = gt_boxes
  blobs['im_info'] = np.array(
    [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],
    dtype=np.float32)

  blobs['img_id'] = roidb[0]['img_id']

  return blobs

def _get_image_blob(roidb, scale_inds):
  """Builds an input blob from the images in the roidb at the specified
  scales.
  """
  num_images = len(roidb)

  processed_ims = []
  im_scales = []
  for i in range(num_images):
    im = cv2.imread(roidb[i]['image'])

    # if len(im.shape) == 2:
    #   im = im[:,:,np.newaxis]
      #im = np.concatenate((im,im,im), axis=2)
    # flip the channel, since the original one using cv2
    # rgb -> bgr
    #im = im[:,:,::-1]

    # if roidb[i]['flipped']:
    #   im = im[:, ::-1, :]
    im, im_scale = prep_im_for_blob(im, 512)
    im_scales.append(im_scale)
    processed_ims.append(im)

  # Create a blob to hold the input images
  blob = im_list_to_blob(processed_ims)

  return blob, im_scales

def im_list_to_blob(ims):
    """Convert a list of images into a network input.

    Assumes images are already prepared (means subtracted, BGR order, ...).
    """
    max_shape = np.array([im.shape for im in ims]).max(axis=0)
    num_images = len(ims)
    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),
                    dtype=np.float32)
    for i in range(num_images):
        im = ims[i]
        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im

    return blob

def prep_im_for_blob(im, target_size):
    """Mean subtract and scale an image for use in a blob."""

    im = im.astype(np.float32, copy=False)
    im = np.divide(im, 255.0)
    im_size_min = im.shape[1]
    im_scale = float(target_size) / float(im_size_min)
    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,
                    interpolation=cv2.INTER_LINEAR)

    return im, im_scale

def combined_roidb(imdb_name, training=True):

  imdb = wider_face(imdb_name)
  imdb.set_proposal_method('gt')
  prepare_roidb(imdb)
  roidb = imdb.roidb

  if training:
    roidb = filter_roidb(roidb)

  ratio_list, ratio_index = rank_roidb_ratio(roidb)

  return imdb, roidb, ratio_list, ratio_index

def prepare_roidb(imdb):
  """Enrich the imdb's roidb by adding some derived quantities that
  are useful for training. This function precomputes the maximum
  overlap, taken over ground-truth boxes, between each ROI and
  each ground-truth box. The class with maximum overlap is also
  recorded.
  """

  roidb = imdb.roidb
  # if not (imdb.name.startswith('coco')):
  #   sizes = [PIL.Image.open(imdb.image_path_at(i)).size
  #        for i in range(imdb.num_images)]
  sizes = np.load('data/sizes.npy')
         
  for i in range(len(imdb.image_index)):
    roidb[i]['img_id'] = imdb.image_id_at(i)
    roidb[i]['image'] = imdb.image_path_at(i)
    if not (imdb.name.startswith('coco')):
      roidb[i]['width'] = sizes[i][0]
      roidb[i]['height'] = sizes[i][1]
    # # need gt_overlaps as a dense array for argmax
    # gt_overlaps = roidb[i]['gt_overlaps'].toarray()
    # # max overlap with gt over classes (columns)
    # max_overlaps = gt_overlaps.max(axis=1)
    # # gt class that had the max overlap
    # max_classes = gt_overlaps.argmax(axis=1)
    # roidb[i]['max_classes'] = max_classes
    # roidb[i]['max_overlaps'] = max_overlaps
    # # sanity checks
    # # max overlap of 0 => class should be zero (background)
    # zero_inds = np.where(max_overlaps == 0)[0]
    # assert all(max_classes[zero_inds] == 0)
    # # max overlap > 0 => class should not be zero (must be a fg class)
    # nonzero_inds = np.where(max_overlaps > 0)[0]
    # assert all(max_classes[nonzero_inds] != 0)
    
def rank_roidb_ratio(roidb):
    # rank roidb based on the ratio between width and height.
    ratio_large = 2 # largest ratio to preserve.
    ratio_small = 0.5 # smallest ratio to preserve.    
    
    ratio_list = []
    for i in range(len(roidb)):
      width = roidb[i]['width']
      height = roidb[i]['height']
      ratio = width / float(height)

      if ratio > ratio_large:
        roidb[i]['need_crop'] = 1
        ratio = ratio_large
      elif ratio < ratio_small:
        roidb[i]['need_crop'] = 1
        ratio = ratio_small        
      else:
        roidb[i]['need_crop'] = 0

      ratio_list.append(ratio)

    ratio_list = np.array(ratio_list)
    ratio_index = np.argsort(ratio_list)
    return ratio_list[ratio_index], ratio_index

def filter_roidb(roidb):
    # filter the image without bounding box.
    i = 0
    while i < len(roidb):
      if len(roidb[i]['boxes']) == 0:
        del roidb[i]
        i -= 1
      i += 1

    return roidb

import torch
import numpy as np
import torch.nn as nn
import torchvision
import torch.nn.functional as F


class RPNFeatureExtractor(nn.Module):
    def __init__(self):
        super(RPNFeatureExtractor, self).__init__()
        self.name = "rpnFeatureExtractor"
        
        #use a dummy image to test when the output size of vgg network is below 50 and eliminate the rest
        #of the layers
        dummy_img = torch.zeros((1, 3, 512, 512)).float()#NOTE 210 is the size of the input image here
        #print(dummy_img)
        #make the model
        model = torchvision.models.vgg16(pretrained=True)
        fe = list(model.features)
        #print(fe) # length is 15
        #pass the dummy image through the layers 
        req_features = []
        k = dummy_img.clone()
        for i in fe:
            k = i(k)
            if k.size()[2] < 50:
                break#trim off the remaining features with the incorrect output size
            req_features.append(i)#in this case it become 52*52 image with 256 input
            #out_channels = k.size()[1]
        #print(len(req_features)) #30
        #print(out_channels) # 512

        #make the network
        self.faster_rcnn_fe_extractor = nn.Sequential(*req_features)

    def forward(self, x):
        return self.faster_rcnn_fe_extractor(x)

class RPNmodel(nn.Module):
    def __init__(self):
        super(RPNmodel, self).__init__()
        self.name = "rpn"
        self.featureWidth = 64
        self.featureChannels = 512
        self.imageWidth = 512 #the original image width
        #help from: https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439
        self.mid_channels = 256 #this was originally 256
        self.in_channels = self.featureChannels# Note this used to be 512 but the features are now 256 layers # for the output channels of vgg 16 feature extractor this is equal to 512
        self.n_anchor = 9 # Number of anchors at each location
        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, 3, 1, 1)
        self.reg_layer = nn.Conv2d(self.mid_channels, self.n_anchor*4, 1, 1, 0)
        self.cls_layer = nn.Conv2d(self.mid_channels, self.n_anchor*2, 1, 1, 0) ## I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1.


        # conv sliding layer
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv1.bias.data.zero_()
        # Regression layer
        self.reg_layer.weight.data.normal_(0, 0.01)
        self.reg_layer.bias.data.zero_()
        # classification layer
        self.cls_layer.weight.data.normal_(0, 0.01)
        self.cls_layer.bias.data.zero_()

    def forward(self, x):#x is the feature map
        x = self.conv1(x)
        pred_anchor_locs = self.reg_layer(x)
        pred_cls_scores = self.cls_layer(x)
        #print(pred_cls_scores.shape, pred_anchor_locs.shape)
        #Out:
        #torch.Size([1, 18, 50, 50]) torch.Size([1, 36, 50, 50])

        pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)
        #print(pred_anchor_locs.shape)
        #Out: torch.Size([1, 22500, 4])
        pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()
        #print(pred_cls_scores)
        #Out torch.Size([1, 50, 50, 18])
        objectness_score = pred_cls_scores.view(1, self.featureWidth, self.featureWidth, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)
        #note that this used to be 50 by 50 but now the feature maps are 52 by 52
        #print(objectness_score.shape)
        #Out torch.Size([1, 22500])
        pred_cls_scores  = pred_cls_scores.view(1, -1, 2)
        #print(pred_cls_scores.shape)
        # Out torch.size([1, 22500, 2])
        ROIs = proposeRegions(pred_anchor_locs, objectness_score, self.imageWidth, self.featureWidth)
        return ROIs, pred_anchor_locs, pred_cls_scores, objectness_score

def proposeRegions(pred_anchor_locs, objectness_score, imageWidth, featureWidth):
    anchors = generateAnchors(featureWidth, imageWidth)
    nms_thresh = 0.7
    n_train_pre_nms = 12000
    n_train_post_nms = 2000
    n_test_pre_nms = 6000
    n_test_post_nms = 300
    min_size = 16

    #Convert anchors format from y1, x1, y2, x2 to ctr_x, ctr_y, h, w
    anc_height = anchors[:, 2] - anchors[:, 0]
    anc_width = anchors[:, 3] - anchors[:, 1]
    anc_ctr_y = anchors[:, 0] + 0.5 * anc_height
    anc_ctr_x = anchors[:, 1] + 0.5 * anc_width

    #Convert predictions locs using above formulas
    pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()
    objectness_score_numpy = objectness_score[0].data.numpy()
    dy = pred_anchor_locs_numpy[:, 0::4]
    dx = pred_anchor_locs_numpy[:, 1::4]
    dh = pred_anchor_locs_numpy[:, 2::4]
    dw = pred_anchor_locs_numpy[:, 3::4]
    ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]
    ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]
    h = np.exp(dh) * anc_height[:, np.newaxis]
    w = np.exp(dw) * anc_width[:, np.newaxis]

    #convert [ctr_x, ctr_y, h, w] to [y1, x1, y2, x2] format
    roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=pred_anchor_locs_numpy.dtype)
    roi[:, 0::4] = ctr_y - 0.5 * h
    roi[:, 1::4] = ctr_x - 0.5 * w
    roi[:, 2::4] = ctr_y + 0.5 * h
    roi[:, 3::4] = ctr_x + 0.5 * w

    #clip the predicted boxes to the image
    img_size = (imageWidth, imageWidth) #Image size
    roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])
    roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])
    #print(roi)
    ######
    #Remove predicted boxes with either height or width < threshold
    hs = roi[:, 2] - roi[:, 0]
    ws = roi[:, 3] - roi[:, 1]
    keep = np.where((hs >= min_size) & (ws >= min_size))[0]
    roi = roi[keep, :]
    score = objectness_score_numpy[keep]
    #print(score.shape)
    #Out:
    ##(22500, ) all the boxes have minimum size of 16

    #I THINK THIS IS AN ERROR IN THE TUTORIAL... RAVEL AND THEN ARGSORT PRODUCES INDICES GREATER THAN THE LENGTH OF THE ORIGINAL ARRAY
    #Sort all (proposal, score) pairs by score from highest to lowest
    order = score.ravel().argsort()[::-1]
    #DOING THIS INSTEAD:
    #for i in score:
    #print(order)

    #order = order[order < len(roi)]#remove indices that are larger than the length of the roi array

    #Take top pre_nms_topN
    #if the length of roi is less than the length of pre_nms then use it to avoid an error
    order = order[:min(n_train_pre_nms,len(roi), len(order))]#THIS CAN BE SWITCHED TO TEST INSTEAD OF TRAIN
   
    #roi = roi[order, :]
    #print(roi.shape)
    #print(roi)

    #####
    #
    y1 = roi[:, 0]
    x1 = roi[:, 1]
    y2 = roi[:, 2]
    x2 = roi[:, 3]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    #order = score.argsort()[::-1]#commented this from the tutorial
    keep = []
    while order.size > 0:
        i = order[0]
        keep += [i]#NOTE THAT THE TUTORIAL HAS A MISTAKE HERE IT FORGOT TO ADD TO THE keep ARRAY
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        inds = np.where(ovr <= nms_thresh)[0]
        order = order[inds + 1]
        

    keep = keep[:min(n_train_post_nms, len(roi), len(keep))] # while training/testing , use accordingly
    roi = roi[keep] # the final region proposals
    return roi

def generateAnchors(featureWidth, imageWidth):
    sub_sample = int(imageWidth/featureWidth)#sub_sample is the ratio of the image width to the feature map width

    ratios = [0.5, 1, 2]
    anchor_scales = [4, 8, 16]
    '''
    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)
    #print(anchor_base)
    
    #make the anchor boundaries:
    ctr_y = sub_sample / 2.
    ctr_x = sub_sample / 2.
    #print(ctr_y, ctr_x)
    # Out: (8, 8)
    for i in range(len(ratios)):
        for j in range(len(anchor_scales)):
            h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])
            w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])
            index = i * len(anchor_scales) + j
            anchor_base[index, 0] = ctr_y - h / 2.
            anchor_base[index, 1] = ctr_x - w / 2.
            anchor_base[index, 2] = ctr_y + h / 2.
            anchor_base[index, 3] = ctr_x + w / 2.
    '''
    #generating anchor points at the feature map locations
    #fe_size = (800//16)
    ctr_x = np.arange(sub_sample, (featureWidth+1) * sub_sample, sub_sample)
    ctr_y = np.arange(sub_sample, (featureWidth+1) * sub_sample, sub_sample)

    ctr = []
    index = 0
    for x in range(len(ctr_x)):
        for y in range(len(ctr_y)):
            ctr += [[ctr_x[x] - int(sub_sample/2), ctr_y[y] - int(sub_sample/2)]]
            index +=1

    anchors = np.zeros((featureWidth * featureWidth * len(ratios)*len(anchor_scales), 4))
    index = 0
    for c in ctr:
        ctr_y, ctr_x = c
        for i in range(len(ratios)):
            for j in range(len(anchor_scales)):
                h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])
                w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])
                anchors[index, 0] = ctr_y - h / 2.
                anchors[index, 1] = ctr_x - w / 2.
                anchors[index, 2] = ctr_y + h / 2.
                anchors[index, 3] = ctr_x + w / 2.
                index += 1
    #print(anchors)
    return anchors

#this training loss function based on the same tutorial as mentioned above
def training_loss(imageWidth, featureWidth, rpnModel, trainingFeatureMapInput, boundaryBoxGroundTruths, numBoxes):
    #TRAINING STUFF:
    anchors = generateAnchors(featureWidth, imageWidth)
    #creating arbitrary bounding boxes

    print("first",boundaryBoxGroundTruths)
    boundaryBoxGroundTruths = boundaryBoxGroundTruths.numpy()

    bbox = np.zeros((numBoxes,4),dtype=np.float32)
    for i in range(numBoxes):
      bbox[i,0]=boundaryBoxGroundTruths[0][i][1]
      bbox[i,1]=boundaryBoxGroundTruths[0][i][0]
      bbox[i,2]=boundaryBoxGroundTruths[0][i][3]
      bbox[i,3]=boundaryBoxGroundTruths[0][i][2]
    print("after numpy:",bbox)
    #bbox = np.asarray([[20, 30, 80, 90], [100, 100, 150, 150]], dtype=np.float32) # [y1, x1, y2, x2] format
    
    labels = np.asarray([1, 2], dtype=np.int8) # 0 represents background

    #finding the index of valid anchor boxes
    index_inside = np.where(
            (anchors[:, 0] >= 0) &
            (anchors[:, 1] >= 0) &
            (anchors[:, 2] <= imageWidth) &
            (anchors[:, 3] <= imageWidth)
        )[0]
    #print(index_inside.shape)
    #Out: (8940,)
    #create array with valid anchor boxes
    valid_anchor_boxes = anchors[index_inside]
    #print(valid_anchor_boxes.shape)
    #Out = (8940, 4)

    #create an empty label array with inside_index shape and fill with -1
    label = np.empty((len(index_inside), ), dtype=np.int32)
    label.fill(-1)
    #print(label.shape)
    #Out = (8940, )

    #Create the IOU ARRAY ious
    ious = np.empty((len(valid_anchor_boxes), len(bbox)), dtype=np.float32)#THIS WAS WRONG CHANGED IT TO LEN(BBOX)
    ious.fill(0)
    #print(bbox)
    for num1, anchor_locations in enumerate(valid_anchor_boxes):
        ya1, xa1, ya2, xa2 = anchor_locations
        anchor_area = (ya2 - ya1) * (xa2 - xa1)
        for num2, boundary_locations in enumerate(bbox):
            yb1, xb1, yb2, xb2 = boundary_locations
            box_area = (yb2- yb1) * (xb2 - xb1)
            inter_x1 = max([xb1, xa1])
            inter_y1 = max([yb1, ya1])
            inter_x2 = min([xb2, xa2])
            inter_y2 = min([yb2, ya2])
            if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):
                iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)
                iou = iter_area / (anchor_area+ box_area - iter_area)            
            else:
                iou = 0.
            ious[num1, num2] = iou
    #print(ious.shape)
    #Out: [22500, 2]


    #the highest iou for each gt_box and its corresponding anchor box    
    gt_argmax_ious = ious.argmax(axis=0)
    #print(gt_argmax_ious)#its corresponding anchor box  
    gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]
    #print(gt_max_ious)#the highest iou for each gt_box
    # Out:
    # [2262 5620]
    # [0.68130493 0.61035156]
    
    # the highest iou for each anchor box and its corresponding ground truth box
    argmax_ious = ious.argmax(axis=1)#its corresponding ground truth box
    #print(argmax_ious.shape)#
    #print(argmax_ious)
    max_ious = ious[np.arange(len(index_inside)), argmax_ious]#the maximum ious for each anchor box
    #print(max_ious)
    # Out:
    # (22500,)
    # [0, 1, 0, ..., 1, 0, 0]
    # [0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]

    #finding the anchor boxes which have the highest ios in either bounding box
    gt_argmax_ious = np.where(ious == gt_max_ious)[0]########
    #print(gt_argmax_ious)
    # Out:
    # [2262, 2508, 5620, 5628, 5636, 5644, 5866, 5874, 5882, 5890, 6112,
    #        6120, 6128, 6136, 6358, 6366, 6374, 6382]

    #set up threshold variables
    pos_iou_threshold  = 0.7
    neg_iou_threshold = 0.3

    #assign the labels for whether each anchor box matches well with a ground truth box
    #Assign negitive label (0) to all the anchor boxes which have max_iou less than negitive threshold
    label[max_ious < neg_iou_threshold] = 0
    #Assign positive label (1) to all the anchor boxes which have highest IoU overlap with a ground-truth box
    label[gt_argmax_ious] = 1
    #Assign positive label (1) to all the anchor boxes which have max_iou greater than positive threshold 
    label[max_ious >= pos_iou_threshold] = 1

    ####THE ACTUAL TRAINING
    #setting up some variable
    pos_ratio = 0.5#the ratio of positive and negative labelled samples
    n_sample = 256 #the number of anchors to sample from the image

    n_pos = int(pos_ratio * n_sample)#the total number of positive samples
    
    #randomly sample n_pos samples from the positive labels and ignore (-1) the remaining ones
    #if there are less than n_pos samples, then it will randomly sample (n_sample — n_pos) negitive samples (0) and assign ignore label to the remaining anchor boxes
    #the positive sampling
    pos_index = np.where(label == 1)[0]
    if len(pos_index) > n_pos:
        disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)
        label[disable_index] = -1
    n_neg = n_sample * np.sum(label == 1)
    #the negative sampling
    neg_index = np.where(label == 0)[0]
    if len(neg_index) > n_neg:
        disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)
        label[disable_index] = -1

    #find the locations of the groundtruth object which has max_iou for each anchor
    max_iou_bbox = bbox[argmax_ious]
    #print(max_iou_bbox)

    #convert the y1, x1, y2, x2 format of valid anchor boxes and associated ground truth boxes with max iou to ctr_y, ctr_x , h, w format
    height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]
    width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]
    ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height
    ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width
    
    base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]
    base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]
    base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height
    base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width

    #Use the formulas mentioned in the article to assign ground truth to anchor boxes (ie find the ideal output values of the rpn given the anchor box locations and ground truth locations)
    eps = np.finfo(height.dtype).eps
    height = np.maximum(height, eps)
    width = np.maximum(width, eps)
    dy = (base_ctr_y - ctr_y) / height
    dx = (base_ctr_x - ctr_x) / width
    dh = np.log(base_height / height)
    dw = np.log(base_width / width)
    anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()
    #print(anchor_locs)
    #Out:
    # [[ 0.5855727   2.3091455   0.7415673   1.647276  ]
    #  [ 0.49718437  2.3091455   0.7415673   1.647276  ]
    #  [ 0.40879607  2.3091455   0.7415673   1.647276  ]
    #  ...
    #  [-2.50802    -5.292254    0.7415677   1.6472763 ]
    #  [-2.5964084  -5.292254    0.7415677   1.6472763 ]
    #  [-2.6847968  -5.292254    0.7415677   1.6472763 ]]

    #find the final anchor labels
    anchor_labels = np.empty((len(anchors),), dtype=label.dtype)
    anchor_labels.fill(-1)
    anchor_labels[index_inside] = label
    #find the final anchor locations
    anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)
    anchor_locations.fill(0)
    anchor_locations[index_inside, :] = anchor_locs


    ### THE ACTUAL TRAINING... AKA COMPUTE THE LOSS
    ROIs, pred_anchor_locs, pred_cls_scores, objectness_score = rpnModel(trainingFeatureMapInput)

    '''
    print(pred_anchor_locs.shape)
    print(pred_cls_scores.shape)
    print(anchor_locations.shape)
    print(anchor_labels.shape)
    '''

    #some minor rearranging
    rpn_loc = pred_anchor_locs[0]
    rpn_score = pred_cls_scores[0]
    gt_rpn_loc = torch.from_numpy(anchor_locations.astype(np.float32))
    gt_rpn_score = torch.from_numpy(anchor_labels)
    '''
    print("rpnlocshape",rpn_loc.shape)
    print("rpnscoreshape", rpn_score.shape)
    print("rpnlocshape", gt_rpn_loc.shape)
    print("gtrpnscoreshape",gt_rpn_score.shape)
    '''


    #calculate the classification loss
    rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long(), ignore_index = -1)
    #print(rpn_cls_loss)
    #Out:
    # Variable containing:
    #  0.6940
    # [torch.FloatTensor of size 1]
    #Out
    # torch.Size([12321, 4]) torch.Size([12321, 2]) torch.Size([12321, 4]) torch.Size([12321])

    #Regression loss is applied to the bounding boxes which have positive labels
    pos = gt_rpn_score > 0
    mask = pos.unsqueeze(1).expand_as(rpn_loc)
    #print(mask.shape)
    #Out:
    # torch.Size(12321, 4)

    #select positive label bounding boxes
    mask_loc_preds = rpn_loc[mask].view(-1, 4)
    mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)
    #print(mask_loc_preds.shape, mask_loc_preds.shape)
    #Out:
    # torch.Size([6, 4]) torch.Size([6, 4])

    #find the location (regression) loss
    '''
    x = torch.abs(mask_loc_targets.double() - mask_loc_preds.double())
    rpn_loc_loss = ((x < 1).double() * 0.5 * x**2) + ((x >= 1).double() * (x-0.5))
    '''
    x = torch.abs(mask_loc_targets - mask_loc_preds)
    rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))
    #print(rpn_loc_loss.sum())
    #Out:
    # Variable containing:
    #  0.3826
    # [torch.FloatTensor of size 1]

    #find total loss. class loss is applied on all the bounding boxes and regression loss is applied only positive bounding box
    rpn_lambda = 10.
    N_reg = (gt_rpn_score >0).sum()#used to be .double().sum()
    rpn_loc_loss = rpn_loc_loss.sum() / N_reg
    rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)
    #print(rpn_loss)
    #Out:0.00248
    return rpn_loss



import matplotlib.pyplot as plt


def train(feature_extractor, rpnModel, num_epochs, learning_rate, batch_size):
  
    optimizer = torch.optim.Adam(rpnModel.parameters(), lr=learning_rate)
    
    #val_loss = np.zeros(num_epochs)
    train_loss = np.zeros(num_epochs)
    train_acc = []
    val_acc = []
    acc_no =0
    n = 0
    losses = []
    iters, train_acc = [],[]
    #val_acc = [],[],[]    
    
    imdb, roidb, ratio_list, ratio_index = combined_roidb('train')
    train_size = len(roidb)
    sampler_batch = sampler(train_size, 1)

    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, imdb.num_classes, training=True)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler_batch, num_workers=0)

    imageWidth = 512
    featureWidth = 64
    for epoch in range(num_epochs):
        for imgs, im_info, gt_boxes, num_boxes in dataloader:
            
            trainingFeatureMapInput = feature_extractor(imgs) #Get feature map
            #print(trainingFeatureMapInput.shape())
            #print(gt_boxes)
            loss = training_loss(imageWidth, featureWidth, rpnModel, trainingFeatureMapInput, gt_boxes, num_boxes) #find loss and ROIs
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            #losses += loss.item()
            losses += [float(loss)]
            print("loss per iteration", losses)
            n = n+1
            plot(n, losses)
        #Save model
        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)
        torch.save(net.state_dict(), model_path)
            
        #train_loss[epoch] = losses/(len(trainset)/batch_size)
        #val_loss[epoch] = get_loss_validation(val,rpnModel, batch_size)
        #losses=0.0
        #print('Epoch:{}, Training Loss:{:.4f}'.format(epoch+1, train_loss[epoch])) #Validation Loss:{:.4f}'.format(epoch+1, train_loss[epoch], val_loss))
        
    #plot(n, train_loss)

def plot(n,losses):
    plt.title("Training Curve")
    plt.plot(range(1,n+1), losses, label="Train")
    #plt.plot(range(1,num_epochs+1), val_loss, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.show()

def get_loss_validation(val, rpnModel, batch_size):
  val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size,
                                              num_workers=1)
  for data in train_loader:
    trainingFeatureMapInput = RPNFeatureExtractor(data) #Get feature map
    loss = training(imageWidth, featureWidth, rpnModel, trainingFeatureMapInput, boundaryBoxGroundTruths) #find loss 
            
    losses += loss.item()

    val_loss = float(losses)/(len(val)/batch_size)
  return val_loss

def get_model_name(name, batch_size, learning_rate, epoch):
    """ Generate a name for the model consisting of all the hyperparameter values

    Args:
        config: Configuration object containing the hyperparameters
    Returns:
        path: A string with the hyperparameter name and value concatenated
    """
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,
                                                   batch_size,
                                                   learning_rate,
                                                   epoch)
    return path

rpnModel = RPNmodel()
feature_extractor = RPNFeatureExtractor()

train(feature_extractor, rpnModel, 1, 0.01, 1)